{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97a6ec71-9814-4628-b099-31f88f9ddf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {0: u'__background__',\n",
    " 1: u'person',\n",
    " 2: u'bicycle',\n",
    " 3: u'car',\n",
    " 4: u'motorcycle',\n",
    " 5: u'airplane',\n",
    " 6: u'bus',\n",
    " 7: u'train',\n",
    " 8: u'truck',\n",
    " 9: u'boat',\n",
    " 10: u'traffic light',\n",
    " 11: u'fire hydrant',\n",
    " 12: u'stop sign',\n",
    " 13: u'parking meter',\n",
    " 14: u'bench',\n",
    " 15: u'bird',\n",
    " 16: u'cat',\n",
    " 17: u'dog',\n",
    " 18: u'horse',\n",
    " 19: u'sheep',\n",
    " 20: u'cow',\n",
    " 21: u'elephant',\n",
    " 22: u'bear',\n",
    " 23: u'zebra',\n",
    " 24: u'giraffe',\n",
    " 25: u'backpack',\n",
    " 26: u'umbrella',\n",
    " 27: u'handbag',\n",
    " 28: u'tie',\n",
    " 29: u'suitcase',\n",
    " 30: u'frisbee',\n",
    " 31: u'skis',\n",
    " 32: u'snowboard',\n",
    " 33: u'sports ball',\n",
    " 34: u'kite',\n",
    " 35: u'baseball bat',\n",
    " 36: u'baseball glove',\n",
    " 37: u'skateboard',\n",
    " 38: u'surfboard',\n",
    " 39: u'tennis racket',\n",
    " 40: u'bottle',\n",
    " 41: u'wine glass',\n",
    " 42: u'cup',\n",
    " 43: u'fork',\n",
    " 44: u'knife',\n",
    " 45: u'spoon',\n",
    " 46: u'bowl',\n",
    " 47: u'banana',\n",
    " 48: u'apple',\n",
    " 49: u'sandwich',\n",
    " 50: u'orange',\n",
    " 51: u'broccoli',\n",
    " 52: u'carrot',\n",
    " 53: u'hot dog',\n",
    " 54: u'pizza',\n",
    " 55: u'donut',\n",
    " 56: u'cake',\n",
    " 57: u'chair',\n",
    " 58: u'couch',\n",
    " 59: u'potted plant',\n",
    " 60: u'bed',\n",
    " 61: u'dining table',\n",
    " 62: u'toilet',\n",
    " 63: u'tv',\n",
    " 64: u'laptop',\n",
    " 65: u'mouse',\n",
    " 66: u'remote',\n",
    " 67: u'keyboard',\n",
    " 68: u'cell phone',\n",
    " 69: u'microwave',\n",
    " 70: u'oven',\n",
    " 71: u'toaster',\n",
    " 72: u'sink',\n",
    " 73: u'refrigerator',\n",
    " 74: u'book',\n",
    " 75: u'clock',\n",
    " 76: u'vase',\n",
    " 77: u'scissors',\n",
    " 78: u'teddy bear',\n",
    " 79: u'hair drier',\n",
    " 80: u'toothbrush'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "609cc518-3ec0-41a9-949e-b98aefdc7fda",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 (no detections), 81.1ms\n",
      "Speed: 4.0ms preprocess, 81.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 75.5ms\n",
      "Speed: 2.0ms preprocess, 75.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 70.6ms\n",
      "Speed: 2.0ms preprocess, 70.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 78.9ms\n",
      "Speed: 2.0ms preprocess, 78.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 69.6ms\n",
      "Speed: 2.0ms preprocess, 69.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 67.5ms\n",
      "Speed: 2.0ms preprocess, 67.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 67.8ms\n",
      "Speed: 1.0ms preprocess, 67.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 68.3ms\n",
      "Speed: 2.8ms preprocess, 68.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 94.8ms\n",
      "Speed: 2.0ms preprocess, 94.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 69.3ms\n",
      "Speed: 2.0ms preprocess, 69.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 82.8ms\n",
      "Speed: 1.0ms preprocess, 82.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 60.7ms\n",
      "Speed: 2.0ms preprocess, 60.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 65.3ms\n",
      "Speed: 2.0ms preprocess, 65.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 73.0ms\n",
      "Speed: 2.0ms preprocess, 73.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 76.1ms\n",
      "Speed: 2.0ms preprocess, 76.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 66.7ms\n",
      "Speed: 9.0ms preprocess, 66.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 70.2ms\n",
      "Speed: 1.0ms preprocess, 70.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 84.3ms\n",
      "Speed: 2.0ms preprocess, 84.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 82.6ms\n",
      "Speed: 2.0ms preprocess, 82.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 81.6ms\n",
      "Speed: 2.2ms preprocess, 81.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 81.0ms\n",
      "Speed: 2.0ms preprocess, 81.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 72.9ms\n",
      "Speed: 2.0ms preprocess, 72.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 77.7ms\n",
      "Speed: 2.0ms preprocess, 77.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 76.0ms\n",
      "Speed: 1.7ms preprocess, 76.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 75.5ms\n",
      "Speed: 2.0ms preprocess, 75.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 74.4ms\n",
      "Speed: 1.8ms preprocess, 74.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 77.2ms\n",
      "Speed: 2.0ms preprocess, 77.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 72.8ms\n",
      "Speed: 2.0ms preprocess, 72.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 75.8ms\n",
      "Speed: 2.0ms preprocess, 75.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 76.8ms\n",
      "Speed: 2.0ms preprocess, 76.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 75.8ms\n",
      "Speed: 1.0ms preprocess, 75.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 79.8ms\n",
      "Speed: 7.0ms preprocess, 79.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 80.8ms\n",
      "Speed: 2.0ms preprocess, 80.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 74.8ms\n",
      "Speed: 2.0ms preprocess, 74.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 76.8ms\n",
      "Speed: 2.0ms preprocess, 76.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 76.8ms\n",
      "Speed: 2.0ms preprocess, 76.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 93.7ms\n",
      "Speed: 2.0ms preprocess, 93.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 78.8ms\n",
      "Speed: 2.0ms preprocess, 78.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Boxes' object has no attribute 'conff'. See valid attributes below.\n\n    A class for storing and manipulating detection boxes.\n\n    Args:\n        boxes (torch.Tensor | numpy.ndarray): A tensor or numpy array containing the detection boxes,\n            with shape (num_boxes, 6). The last two columns should contain confidence and class values.\n        orig_shape (tuple): Original image size, in the format (height, width).\n\n    Attributes:\n        boxes (torch.Tensor | numpy.ndarray): The detection boxes with shape (num_boxes, 6).\n        orig_shape (torch.Tensor | numpy.ndarray): Original image size, in the format (height, width).\n        is_track (bool): True if the boxes also include track IDs, False otherwise.\n\n    Properties:\n        xyxy (torch.Tensor | numpy.ndarray): The boxes in xyxy format.\n        conf (torch.Tensor | numpy.ndarray): The confidence values of the boxes.\n        cls (torch.Tensor | numpy.ndarray): The class values of the boxes.\n        id (torch.Tensor | numpy.ndarray): The track IDs of the boxes (if available).\n        xywh (torch.Tensor | numpy.ndarray): The boxes in xywh format.\n        xyxyn (torch.Tensor | numpy.ndarray): The boxes in xyxy format normalized by original image size.\n        xywhn (torch.Tensor | numpy.ndarray): The boxes in xywh format normalized by original image size.\n        data (torch.Tensor): The raw bboxes tensor\n\n    Methods:\n        cpu(): Move the object to CPU memory.\n        numpy(): Convert the object to a numpy array.\n        cuda(): Move the object to CUDA memory.\n        to(*args, **kwargs): Move the object to the specified device.\n        pandas(): Convert the object to a pandas DataFrame (not yet implemented).\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m         x1,y1,x2,y2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(x1),\u001b[38;5;28mint\u001b[39m(y1),\u001b[38;5;28mint\u001b[39m(x2),\u001b[38;5;28mint\u001b[39m(y2)\n\u001b[0;32m     22\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(img,(x1,y1),(x2,y2),(\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m255\u001b[39m),\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m         conf \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil((\u001b[43mbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconff\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     26\u001b[0m         cvzone\u001b[38;5;241m.\u001b[39mputTextRect(img,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m,x1),\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m35\u001b[39m,y1)))\n\u001b[0;32m     28\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m,img)\n",
      "File \u001b[1;32mD:\\Users\\marka\\anaconda3\\envs\\pytorch_ev\\lib\\site-packages\\ultralytics\\utils\\__init__.py:136\u001b[0m, in \u001b[0;36mSimpleClass.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[0;32m    135\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Boxes' object has no attribute 'conff'. See valid attributes below.\n\n    A class for storing and manipulating detection boxes.\n\n    Args:\n        boxes (torch.Tensor | numpy.ndarray): A tensor or numpy array containing the detection boxes,\n            with shape (num_boxes, 6). The last two columns should contain confidence and class values.\n        orig_shape (tuple): Original image size, in the format (height, width).\n\n    Attributes:\n        boxes (torch.Tensor | numpy.ndarray): The detection boxes with shape (num_boxes, 6).\n        orig_shape (torch.Tensor | numpy.ndarray): Original image size, in the format (height, width).\n        is_track (bool): True if the boxes also include track IDs, False otherwise.\n\n    Properties:\n        xyxy (torch.Tensor | numpy.ndarray): The boxes in xyxy format.\n        conf (torch.Tensor | numpy.ndarray): The confidence values of the boxes.\n        cls (torch.Tensor | numpy.ndarray): The class values of the boxes.\n        id (torch.Tensor | numpy.ndarray): The track IDs of the boxes (if available).\n        xywh (torch.Tensor | numpy.ndarray): The boxes in xywh format.\n        xyxyn (torch.Tensor | numpy.ndarray): The boxes in xyxy format normalized by original image size.\n        xywhn (torch.Tensor | numpy.ndarray): The boxes in xywh format normalized by original image size.\n        data (torch.Tensor): The raw bboxes tensor\n\n    Methods:\n        cpu(): Move the object to CPU memory.\n        numpy(): Convert the object to a numpy array.\n        cuda(): Move the object to CUDA memory.\n        to(*args, **kwargs): Move the object to the specified device.\n        pandas(): Convert the object to a pandas DataFrame (not yet implemented).\n    "
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "from ultralytics import YOLO\n",
    "import math\n",
    "\n",
    "model = YOLO('yolov8n.pt')\n",
    "#here's where I will load the local trained model \n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3,1280)\n",
    "cap.set(4,640)\n",
    "\n",
    "while True:\n",
    "    success,img = cap.read()\n",
    "    results = model(img,stream=True)\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            x1,y1,x2,y2 = box.xyxy[0]\n",
    "            x1,y1,x2,y2 = int(x1),int(y1),int(x2),int(y2)\n",
    "            cv2.rectangle(img,(x1,y1),(x2,y2),(255,0,255),3)\n",
    "            \n",
    "            conf = math.ceil((box.conff[0]*100)/100)\n",
    "\n",
    "            cvzone.putTextRect(img,f'{conf}',(max(0,x1),max(35,y1)))\n",
    "            \n",
    "    cv2.imshow('image',img)\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaffb62-5f6e-4db5-8778-4334fa2e39ae",
   "metadata": {},
   "source": [
    "# Document \n",
    "## Document \n",
    "### Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031ca992-5dc8-4d0c-859f-b55e0fd2c766",
   "metadata": {},
   "source": [
    "1- take a look at the class inbalances , consider splitting images with more than one class and then plot the classes \n",
    "and their distribution.  \n",
    "2- research handling unbalanced classes (torch.utils.data.WeightedRandomSampler or data augmentations)  \n",
    "3- it seems that I can't do data augmentaion on the fly so we'll have to do it before feeding the data into the ultralytics model training\n",
    "4- decide which augmentations to do \n",
    "5- have the code run locally that\n",
    "> -create a fully new environment to keep track of the libraries used  \n",
    "> -get the code to automatically load the data from kaggle  \n",
    "> -do the augmentation needed then move the data to YOLO format then do the training  \n",
    "> -get the code to a vm and run the training  \n",
    "> -export the model and load it locally \n",
    "> -give the model video or img input and see how it does \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tempenv",
   "language": "python",
   "name": "tempenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
